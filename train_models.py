"""
Complete Training Pipeline for NASA Exoplanet Detection System
Run this script to train all models from scratch
"""

import os
import sys
import time
import numpy as np
import pandas as pd
from data_acquisition import ExoplanetDataAcquisition
from data_preprocessing import ExoplanetPreprocessor
from models import ExoplanetModelEnsemble
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import classification_report, confusion_matrix
import warnings
warnings.filterwarnings('ignore')

def create_directories():
    """Create necessary directories"""
    directories = ['data', 'data/kepler', 'data/tess', 'data/processed', 'models', 'plots']
    for directory in directories:
        os.makedirs(directory, exist_ok=True)
    print("‚úì Created project directories")

def acquire_data():
    """Acquire and prepare training data"""
    print("\n" + "="*50)
    print("STEP 1: DATA ACQUISITION")
    print("="*50)
    
    data_acq = ExoplanetDataAcquisition()
    
    # Create training dataset
    print("Creating comprehensive training dataset...")
    metadata_df = data_acq.create_training_dataset()
    
    print(f"‚úì Dataset created with {len(metadata_df)} light curves")
    print(f"  - Exoplanet hosts: {metadata_df['has_exoplanet'].sum()}")
    print(f"  - Non-exoplanet: {(~metadata_df['has_exoplanet']).sum()}")
    
    return metadata_df

def preprocess_data(metadata_df):
    """Preprocess data for training"""
    print("\n" + "="*50)
    print("STEP 2: DATA PREPROCESSING")
    print("="*50)
    
    preprocessor = ExoplanetPreprocessor(target_length=2048)
    
    # Process dataset
    print("Processing light curves...")
    processed_data = preprocessor.process_dataset(metadata_df)
    
    # Prepare training splits
    print("Creating train/validation/test splits...")
    data_splits = preprocessor.prepare_training_data(processed_data)
    
    # Save processed data
    preprocessor.save_processed_data(processed_data, data_splits)
    
    print(f"‚úì Processed {len(processed_data['labels'])} sequences")
    print(f"  - Training: {len(data_splits['train']['labels'])}")
    print(f"  - Validation: {len(data_splits['val']['labels'])}")
    print(f"  - Test: {len(data_splits['test']['labels'])}")
    
    return data_splits, preprocessor

def train_models(data_splits):
    """Train all ML models"""
    print("\n" + "="*50)
    print("STEP 3: MODEL TRAINING")
    print("="*50)
    
    # Initialize model ensemble
    n_features = data_splits['train']['features'].shape[1]
    ensemble = ExoplanetModelEnsemble(n_features=n_features)
    
    # Train deep learning models
    print("Training deep learning models...")
    start_time = time.time()
    ensemble.train_deep_models(data_splits, epochs=30, batch_size=32)
    dl_time = time.time() - start_time
    
    # Train classical models
    print("\nTraining classical ML models...")
    start_time = time.time()
    ensemble.train_classical_models(data_splits)
    classical_time = time.time() - start_time
    
    print(f"‚úì Deep learning training time: {dl_time:.1f}s")
    print(f"‚úì Classical ML training time: {classical_time:.1f}s")
    
    return ensemble

def evaluate_models(ensemble, data_splits):
    """Evaluate all models and create ensemble"""
    print("\n" + "="*50)
    print("STEP 4: MODEL EVALUATION")
    print("="*50)
    
    # Evaluate individual models
    results = ensemble.evaluate_models(data_splits)
    
    # Create ensemble prediction
    ensemble_pred = ensemble.create_ensemble_prediction(results, data_splits)
    
    # Print detailed results
    print("\nDetailed Model Performance:")
    print("-" * 80)
    print(f"{'Model':<15} {'Accuracy':<10} {'AUC':<8} {'Precision':<10} {'Recall':<8} {'F1':<8} {'Speed(ms)':<10}")
    print("-" * 80)
    
    for model_name, metrics in results.items():
        speed_ms = metrics.get('inference_time', 0) * 1000
        print(f"{model_name:<15} {metrics['accuracy']:<10.4f} {metrics['auc']:<8.4f} "
              f"{metrics['precision']:<10.4f} {metrics['recall']:<8.4f} "
              f"{metrics['f1']:<8.4f} {speed_ms:<10.1f}")
    
    return results

def create_visualizations(results, data_splits):
    """Create performance visualizations"""
    print("\n" + "="*50)
    print("STEP 5: CREATING VISUALIZATIONS")
    print("="*50)
    
    # Set style
    plt.style.use('dark_background')
    sns.set_palette("husl")
    
    # 1. Model comparison bar chart
    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 12))
    fig.suptitle('NASA Exoplanet Detection - Model Performance Analysis', fontsize=16, fontweight='bold')
    
    # Accuracy comparison
    models = list(results.keys())
    accuracies = [results[model]['accuracy'] for model in models]
    colors = plt.cm.viridis(np.linspace(0, 1, len(models)))
    
    bars1 = ax1.bar(models, accuracies, color=colors)
    ax1.set_title('Model Accuracy Comparison')
    ax1.set_ylabel('Accuracy')
    ax1.set_ylim(0.8, 1.0)
    
    # Add value labels on bars
    for bar, acc in zip(bars1, accuracies):
        ax1.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.005,
                f'{acc:.3f}', ha='center', va='bottom')
    
    # AUC comparison
    aucs = [results[model]['auc'] for model in models]
    bars2 = ax2.bar(models, aucs, color=colors)
    ax2.set_title('Model AUC Comparison')
    ax2.set_ylabel('AUC Score')
    ax2.set_ylim(0.9, 1.0)
    
    for bar, auc in zip(bars2, aucs):
        ax2.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.002,
                f'{auc:.3f}', ha='center', va='bottom')
    
    # Inference time comparison
    times = [results[model].get('inference_time', 0) * 1000 for model in models]
    bars3 = ax3.bar(models, times, color=colors)
    ax3.set_title('Inference Time Comparison')
    ax3.set_ylabel('Time (ms)')
    ax3.set_yscale('log')
    
    for bar, time_ms in zip(bars3, times):
        ax3.text(bar.get_x() + bar.get_width()/2, bar.get_height() * 1.1,
                f'{time_ms:.1f}ms', ha='center', va='bottom')
    
    # Precision vs Recall scatter
    precisions = [results[model]['precision'] for model in models]
    recalls = [results[model]['recall'] for model in models]
    
    scatter = ax4.scatter(precisions, recalls, c=range(len(models)), 
                         cmap='viridis', s=100, alpha=0.7)
    ax4.set_xlabel('Precision')
    ax4.set_ylabel('Recall')
    ax4.set_title('Precision vs Recall')
    
    # Add model labels
    for i, model in enumerate(models):
        ax4.annotate(model, (precisions[i], recalls[i]), 
                    xytext=(5, 5), textcoords='offset points')
    
    plt.tight_layout()
    plt.savefig('plots/model_performance_analysis.png', dpi=300, bbox_inches='tight')
    plt.close()
    
    # 2. Confusion matrix for best model
    best_model = max(results.keys(), key=lambda x: results[x]['auc'])
    
    # Create sample confusion matrix (since we don't have actual predictions stored)
    y_test = data_splits['test']['labels']
    n_samples = len(y_test)
    n_positive = np.sum(y_test)
    n_negative = n_samples - n_positive
    
    # Estimate confusion matrix based on performance metrics
    accuracy = results[best_model]['accuracy']
    precision = results[best_model]['precision']
    recall = results[best_model]['recall']
    
    tp = int(recall * n_positive)
    fp = int(tp / precision - tp) if precision > 0 else 0
    fn = n_positive - tp
    tn = n_negative - fp
    
    cm = np.array([[tn, fp], [fn, tp]])
    
    plt.figure(figsize=(8, 6))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
                xticklabels=['No Exoplanet', 'Exoplanet'],
                yticklabels=['No Exoplanet', 'Exoplanet'])
    plt.title(f'Confusion Matrix - {best_model.title()} Model')
    plt.ylabel('Actual')
    plt.xlabel('Predicted')
    plt.savefig('plots/confusion_matrix.png', dpi=300, bbox_inches='tight')
    plt.close()
    
    print("‚úì Created performance visualizations")
    print(f"  - Model comparison: plots/model_performance_analysis.png")
    print(f"  - Confusion matrix: plots/confusion_matrix.png")

def save_final_results(ensemble, results):
    """Save final models and results"""
    print("\n" + "="*50)
    print("STEP 6: SAVING RESULTS")
    print("="*50)
    
    # Save models
    ensemble.save_models()
    
    # Save results summary
    results_df = pd.DataFrame(results).T
    results_df.to_csv('models/model_performance_summary.csv')
    
    # Create final report
    report = f"""
NASA Exoplanet Detection System - Training Report
================================================

Training completed successfully!

Best Performing Models:
- Highest Accuracy: {max(results.keys(), key=lambda x: results[x]['accuracy'])} ({results[max(results.keys(), key=lambda x: results[x]['accuracy'])]['accuracy']:.4f})
- Highest AUC: {max(results.keys(), key=lambda x: results[x]['auc'])} ({results[max(results.keys(), key=lambda x: results[x]['auc'])]['auc']:.4f})
- Fastest Inference: {min(results.keys(), key=lambda x: results[x].get('inference_time', float('inf')))} ({results[min(results.keys(), key=lambda x: results[x].get('inference_time', float('inf')))].get('inference_time', 0)*1000:.1f}ms)

Ensemble Performance:
- Accuracy: {results['ensemble']['accuracy']:.4f}
- AUC: {results['ensemble']['auc']:.4f}

Files Created:
- models/: Trained model files
- plots/: Performance visualizations
- data/processed/: Preprocessed datasets

Next Steps:
1. Run 'python app.py' to start the web interface
2. Open http://localhost:5000 in your browser
3. Test the live exoplanet detection simulation!
"""
    
    with open('TRAINING_REPORT.txt', 'w') as f:
        f.write(report)
    
    print("‚úì Saved trained models")
    print("‚úì Saved performance summary")
    print("‚úì Created training report")

def main():
    """Main training pipeline"""
    print("üöÄ NASA EXOPLANET DETECTION SYSTEM")
    print("üåü Advanced AI/ML Training Pipeline")
    print("=" * 60)
    
    start_time = time.time()
    
    try:
        # Step 1: Setup
        create_directories()
        
        # Step 2: Data acquisition
        metadata_df = acquire_data()
        
        # Step 3: Data preprocessing
        data_splits, preprocessor = preprocess_data(metadata_df)
        
        # Step 4: Model training
        ensemble = train_models(data_splits)
        
        # Step 5: Model evaluation
        results = evaluate_models(ensemble, data_splits)
        
        # Step 6: Create visualizations
        create_visualizations(results, data_splits)
        
        # Step 7: Save results
        save_final_results(ensemble, results)
        
        total_time = time.time() - start_time
        
        print("\n" + "="*60)
        print("üéâ TRAINING COMPLETED SUCCESSFULLY!")
        print(f"‚è±Ô∏è  Total training time: {total_time:.1f} seconds")
        print("="*60)
        
        print("\nüåê To start the web interface:")
        print("   python app.py")
        print("\nüìä Check the plots/ directory for visualizations")
        print("üìã Read TRAINING_REPORT.txt for detailed results")
        
    except Exception as e:
        print(f"\n‚ùå Training failed with error: {e}")
        print("Please check the error message and try again.")
        sys.exit(1)

if __name__ == "__main__":
    main()
